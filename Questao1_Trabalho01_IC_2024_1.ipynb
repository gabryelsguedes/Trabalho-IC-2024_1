{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 34877,
          "sourceType": "datasetVersion",
          "datasetId": 27352
        }
      ],
      "dockerImageVersionId": 30761,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-10-21T00:15:00.862336Z",
          "iopub.execute_input": "2024-10-21T00:15:00.862785Z",
          "iopub.status.idle": "2024-10-21T00:15:01.327704Z",
          "shell.execute_reply.started": "2024-10-21T00:15:00.862744Z",
          "shell.execute_reply": "2024-10-21T00:15:01.326029Z"
        },
        "trusted": true,
        "id": "Kq1gWCMhFZoS",
        "outputId": "0def6b69-d508-468f-f6d2-a5e588592787"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/mnist-in-csv/mnist_test.csv\n/kaggle/input/mnist-in-csv/mnist_train.csv\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classificador Linear de Mínimos Quadrados"
      ],
      "metadata": {
        "id": "pvYjhj7BFZoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')\n",
        "df_test = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_test.csv')\n",
        "\n",
        "#Divisão dos dados em treino e teste\n",
        "y_train = df_train.iloc[:,0].values\n",
        "x_train = df_train.iloc[:,1:].values\n",
        "y_test = df_test.iloc[:,0].values\n",
        "x_test = df_test.iloc[:,1:].values\n",
        "\n",
        "#y_train = y_train.reshape(-1,1)\n",
        "#x_train = x_train / 255.0 #Normalização\n",
        "#x_test = x_test / 255.0 #Normalização\n",
        "\n",
        "#Criação de uma função para binarizar os rótulos\n",
        "def one_hot_encode(y, num_classes):\n",
        "    return np.eye(num_classes)[y]\n",
        "\n",
        "# Codificar os rótulos de treino e teste\n",
        "y_train_one_hot = one_hot_encode(y_train, 10)\n",
        "y_test_one_hot = one_hot_encode(y_test, 10)\n",
        "\n",
        "# Adicionar uma coluna do coeficiente em B0(intercepto)\n",
        "x_train_bias = np.c_[np.ones((x_train.shape[0], 1)), x_train]\n",
        "x_test_bias = np.c_[np.ones((x_test.shape[0], 1)), x_test]\n",
        "\n",
        "# Regularização\n",
        "lambda_identity = 1e-3 * np.eye(x_train_bias.shape[1])  # Para regularização\n",
        "\n",
        "# Resolver a equação normal: W = (X^T * X)^-1 * X^T * Y\n",
        "B = np.linalg.inv(x_train_bias.T @ x_train_bias + lambda_identity) @ x_train_bias.T @ y_train_one_hot\n",
        "#W = np.linalg.pinv(x_train_bias.T) @ y_train_one_hot\n",
        "\n",
        "y_pred = x_test_bias @ B\n",
        "\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Calcular a precisão\n",
        "accuracy = accuracy_score(y_test, y_pred_labels)\n",
        "print(f\"Acurácia: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T00:15:16.861905Z",
          "iopub.execute_input": "2024-10-21T00:15:16.862457Z",
          "iopub.status.idle": "2024-10-21T00:15:25.171749Z",
          "shell.execute_reply.started": "2024-10-21T00:15:16.862407Z",
          "shell.execute_reply": "2024-10-21T00:15:25.170500Z"
        },
        "trusted": true,
        "id": "_juGINtsFZoW",
        "outputId": "582c123a-64b5-40fa-f9b0-15c2e017f88d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Acurácia: 86.03%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A aplicação do modelo de classificação linear usando os mínimos quadrados gerou uma acurácia de 86%, enquanto o modelo a ser comparado gerou uma acurácia de 88%. Em seguida outros modelos serão comparados com o modelo em questão para analisar a acurácia dos modelos.\n",
        "\n",
        "Durante os testes com o modelo, foi verificado que a matriz era singular, assim, para evitar o problema de singularidade foi utilizado o metodo de regularização de Thikonov.\n",
        "\n",
        "Como o modelo a ser comparado na questão não utiliza pre processamento os dados MNIST também não sofreu nenhuma modificação em seus dados originais.\n",
        "\n",
        "Para fins de estudo, foi aplicada uma normalização nos dados para verificar  se ocorreria alguma melhoria ao aplicar a normalização dos dados, porém o resultado permaneceu o mesmo.\n",
        "\n",
        "Foi realizada a transformação dos dados de saída para o sistema multiclasse. Como o sistema MNIST possui 10 classe, 0 até 9, os rótulos são transformados para um vetor binário  no qual apenas a posição referente ao valor correto recebe o valor de 1 e os outros valores recebem o valor 0.\n",
        "\n"
      ],
      "metadata": {
        "id": "8mSlL_tPFZoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adaline com Função de Ativação Sigmoidal"
      ],
      "metadata": {
        "id": "8zAmJeEtFZoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Carregar os dataframes\n",
        "df_train = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')\n",
        "df_test = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_test.csv')\n",
        "\n",
        "# Separar os rótulos e dados de treino\n",
        "y_train = df_train.iloc[:, 0].values  # A primeira coluna contém os rótulos\n",
        "X_train = df_train.iloc[:, 1:].values #/ 255.0  # Normalização (dividindo por 255.0)\n",
        "\n",
        "# Separar os rótulos e dados de teste\n",
        "y_test = df_test.iloc[:, 0].values  # A primeira coluna contém os rótulos\n",
        "X_test = df_test.iloc[:, 1:].values #/ 255.0  # Normalização (dividindo por 255.0)\n",
        "\n",
        "# Converter rótulos para NumPy arrays e redimensioná-los\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "# Codificação one-hot para os rótulos (necessário para 10 classificadores binários)\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_one_hot = encoder.fit_transform(y_train)\n",
        "y_test_one_hot = encoder.transform(y_test)\n",
        "\n",
        "# Adicionar uma coluna de 1s para o bias em X\n",
        "X_train_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]  # Adicionando coluna de 1s\n",
        "X_test_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]  # Adicionando coluna de 1s\n",
        "\n",
        "# Função sigmoide\n",
        "def sigmoid(z):\n",
        "    \"\"\"Função sigmoide.\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Função para treinar MADALINE com função sigmoide\n",
        "def train_madaline_sigmoid(X, y, epochs=1000, lr=0.01):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_classes = y.shape[1]\n",
        "\n",
        "    # Inicializar os pesos (incluindo o bias como parte do vetor de pesos)\n",
        "    weights = np.random.rand(n_classes, n_features)  # Pesos para cada classe\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Calcular a saída para todas as amostras\n",
        "        z_input = np.dot(X, weights.T)  # Saídas lineares para todas as amostras\n",
        "        z = sigmoid(z_input)  # Aplicar função sigmoide\n",
        "\n",
        "        # Calcular o erro (diferença entre o esperado e o previsto)\n",
        "        error = y - z\n",
        "\n",
        "        # Atualizar os pesos com base no erro\n",
        "        for j in range(n_classes):\n",
        "            # Atualizar pesos apenas para as classes onde houve erro\n",
        "            mask = (y[:, j] > 0) & (z[:, j] <= 0.5) | (y[:, j] <= 0) & (z[:, j] > 0.5)\n",
        "            weights[j] += lr * np.dot(mask * error[:, j], X)  # Atualizar pesos\n",
        "\n",
        "        # Imprimir erro por época\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}: Error = {np.sum(np.square(error)):.4f}\") #Erro quadrático\n",
        "\n",
        "    return weights\n",
        "\n",
        "# Treinar o modelo MADALINE com função sigmoide\n",
        "weights = train_madaline_sigmoid(X_train_bias, y_train_one_hot, epochs=1000, lr=0.01)\n",
        "\n",
        "# Fazer previsões no conjunto de teste\n",
        "y_pred_prob = sigmoid(X_test_bias @ weights.T)  # O bias está incorporado\n",
        "\n",
        "# Converter probabilidades para rótulos (classe com maior probabilidade)\n",
        "y_pred_labels = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "# Calcular a acurácia\n",
        "accuracy = accuracy_score(y_test.flatten(), y_pred_labels)\n",
        "print(f\"Acurácia: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T00:15:46.272366Z",
          "iopub.execute_input": "2024-10-21T00:15:46.272982Z",
          "iopub.status.idle": "2024-10-21T00:20:54.187127Z",
          "shell.execute_reply.started": "2024-10-21T00:15:46.272938Z",
          "shell.execute_reply": "2024-10-21T00:20:54.185925Z"
        },
        "trusted": true,
        "id": "PpwWWS0sFZoX",
        "outputId": "d927b516-9c22-4ccc-98c3-b45a63bac70d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 0: Error = 540000.0000\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_36/701434130.py:34: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 100: Error = 31350.0000\nEpoch 200: Error = 25976.0000\nEpoch 300: Error = 25850.0000\nEpoch 400: Error = 20406.0000\nEpoch 500: Error = 28022.0000\nEpoch 600: Error = 19758.0000\nEpoch 700: Error = 21621.0000\nEpoch 800: Error = 19882.0000\nEpoch 900: Error = 18443.0000\nAcurácia: 86.58%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A aplicação do modelo MADALINE com função de ativação sigmoidal apresentou uma Acurácia de valor próximo ao do modelo linear, com valor de 86,58%. Para o desenvolvimento do modelo foi mantida a não aplicação do pré processamento dos dados, a fim de comparação com o modelo linear e a aplicação dele e do modelo em questão com o PCA.\n",
        "\n",
        "Durante o desenvolvimento do modelo foram realizados testes com o embaralhamento dos dados, porém os resultados apresentados se mantiveram semelhante ao modelo testado sem o embaralhamento. Durante a aplicação da variação da quantidade de épocas foi observada que os melhores valores da acurácia nos dados de teste foram a partir de 500 épocas. As taxas de aprendizagem testadas foram de 0.1, 0.05 e 0.01, o melhor resultado apresentado foi com a taxa de 0.01."
      ],
      "metadata": {
        "id": "n6Xe1beSFZoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA com modelo Adaline e função de ativação Sigmoidal"
      ],
      "metadata": {
        "id": "C7t4cvDMFZoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Carregar os dataframes\n",
        "df_train = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')\n",
        "df_test = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_test.csv')\n",
        "\n",
        "# Separar os rótulos e dados de treino\n",
        "y_train = df_train.iloc[:, 0].values  # A primeira coluna contém os rótulos\n",
        "X_train = df_train.iloc[:, 1:].values / 255.0  # Normalização (dividindo por 255.0)\n",
        "\n",
        "# Separar os rótulos e dados de teste\n",
        "y_test = df_test.iloc[:, 0].values  # A primeira coluna contém os rótulos\n",
        "X_test = df_test.iloc[:, 1:].values / 255.0  # Normalização (dividindo por 255.0)\n",
        "\n",
        "# Converter rótulos para NumPy arrays e redimensioná-los\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "# Codificação one-hot para os rótulos (necessário para 10 classificadores binários)\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_one_hot = encoder.fit_transform(y_train)\n",
        "y_test_one_hot = encoder.transform(y_test)\n",
        "\n",
        "# Aplicar PCA para reduzir a dimensionalidade (preservando 95% da variância)\n",
        "pca = PCA(n_components=0.99)  # ou pode escolher um número fixo, por exemplo, n_components=50\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Adicionar uma coluna de 1s para o bias em X\n",
        "X_train_bias = np.c_[np.ones((X_train_pca.shape[0], 1)), X_train_pca]  # Adicionando coluna de 1s\n",
        "X_test_bias = np.c_[np.ones((X_test_pca.shape[0], 1)), X_test_pca]  # Adicionando coluna de 1s\n",
        "\n",
        "EQM = []\n",
        "# Função sigmoide\n",
        "def sigmoid(z):\n",
        "    \"\"\"Função sigmoide.\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Função para treinar MADALINE com função sigmoide\n",
        "def train_madaline_sigmoid(X, y, epochs=1000, lr=0.01):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_classes = y.shape[1]\n",
        "\n",
        "    # Inicializar os pesos (incluindo o bias como parte do vetor de pesos)\n",
        "    weights = np.random.rand(n_classes, n_features)  # Pesos para cada classe\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Calcular a saída para todas as amostras\n",
        "        z_input = np.dot(X, weights.T)  # Saídas lineares para todas as amostras\n",
        "        z = sigmoid(z_input)  # Aplicar função sigmoide\n",
        "\n",
        "        # Calcular o erro (diferença entre o esperado e o previsto)\n",
        "        error = y - z\n",
        "\n",
        "        # Atualizar os pesos com base no erro\n",
        "        for j in range(n_classes):\n",
        "            # Atualizar pesos apenas para as classes onde houve erro\n",
        "            mask = (y[:, j] > 0) & (z[:, j] <= 0.5) | (y[:, j] <= 0) & (z[:, j] > 0.5)\n",
        "            weights[j] += lr * np.dot(mask * error[:, j], X)  # Atualizar pesos\n",
        "\n",
        "        # Imprimir erro por época\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}: Error = {np.sum(np.square(error)):.4f}\") #Erro quadrático\n",
        "        EQM.append(np.sum(np.square(error)))\n",
        "    return weights, EQM\n",
        "\n",
        "# Treinar o modelo MADALINE com função sigmoide\n",
        "weights, EQM = train_madaline_sigmoid(X_train_bias, y_train_one_hot, epochs=1000, lr=0.01)\n",
        "\n",
        "# Fazer previsões no conjunto de teste\n",
        "y_pred_prob = sigmoid(X_test_bias @ weights.T)  # O bias está incorporado\n",
        "\n",
        "# Converter probabilidades para rótulos (classe com maior probabilidade)\n",
        "y_pred_labels = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "# Calcular a acurácia\n",
        "accuracy = accuracy_score(y_test.flatten(), y_pred_labels)\n",
        "print(f\"Acurácia: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T02:08:57.530679Z",
          "iopub.execute_input": "2024-10-21T02:08:57.531099Z",
          "iopub.status.idle": "2024-10-21T02:11:59.629557Z",
          "shell.execute_reply.started": "2024-10-21T02:08:57.531059Z",
          "shell.execute_reply": "2024-10-21T02:11:59.628357Z"
        },
        "trusted": true,
        "id": "MB-JDUFgFZoY",
        "outputId": "c4b8373e-0ba9-45d5-8808-c533b3e71bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 0: Error = 255943.6697\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_36/801356226.py:41: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-z))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 100: Error = 22270.6747\nEpoch 200: Error = 16235.2463\nEpoch 300: Error = 20172.1377\nEpoch 400: Error = 20757.9162\nEpoch 500: Error = 25312.6827\nEpoch 600: Error = 19081.4735\nEpoch 700: Error = 18625.4405\nEpoch 800: Error = 13038.1918\nEpoch 900: Error = 19762.3195\nAcurácia: 88.48%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A aplicação do PCA com uma retenção de 99% da variância no modelo Adaline trouxe uma melhoria na Acurácia, elevando de aproximadamente 86% para 88,48%. Foram testadas menores retenções como 90% e 85%, porém em ambos os casos o modelo teve uma perda na acurácia se comparado com o modelo Adaline sem a aplicação do PCA, ficando entre 84% e 82%.\n",
        "\n",
        "Uma observação constatada foi que durante os testes o valor do erro tem uma rápida queda, porém após esse primeiro momento o ocorre uma oscilação do erro, esse mesmo fato não ocorre sem o uso do PCA."
      ],
      "metadata": {
        "id": "VkcNn462FZoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA + Classificador linear com Minimos Quadrados"
      ],
      "metadata": {
        "id": "mcCuT5xpFZoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Carregar os dataframes\n",
        "df_train = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')\n",
        "df_test = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_test.csv')\n",
        "\n",
        "# Separar os rótulos e dados de treino\n",
        "y_train = df_train.iloc[:, 0].values  # A primeira coluna contém os rótulos\n",
        "X_train = df_train.iloc[:, 1:].values / 255.0  # As demais colunas contêm os dados (normalização)\n",
        "\n",
        "# Separar os rótulos e dados de teste\n",
        "y_test = df_test.iloc[:, 0].values  # A primeira coluna contém os rótulos\n",
        "X_test = df_test.iloc[:, 1:].values / 255.0  # As demais colunas contêm os dados (normalização)\n",
        "\n",
        "# Aplicar PCA para reduzir a dimensionalidade (preservando 95% da variância)\n",
        "pca = PCA(n_components=0.94)  # PCA manterá componentes suficientes para explicar 95% da variância\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Codificar os rótulos de treino e teste com one-hot encoding\n",
        "def one_hot_encode(y, num_classes):\n",
        "    return np.eye(num_classes)[y]\n",
        "\n",
        "y_train_one_hot = one_hot_encode(y_train, 10)\n",
        "y_test_one_hot = one_hot_encode(y_test, 10)\n",
        "\n",
        "# Adicionar uma coluna de bias aos dados com PCA\n",
        "X_train_bias = np.c_[np.ones((X_train_pca.shape[0], 1)), X_train_pca]\n",
        "X_test_bias = np.c_[np.ones((X_test_pca.shape[0], 1)), X_test_pca]\n",
        "\n",
        "# Regularização\n",
        "lambda_identity = 1e-3 * np.eye(X_train_bias.shape[1])  # Para regularização (lambda = 0.001)\n",
        "\n",
        "# Resolver a equação normal: W = (X^T * X + lambda*I)^-1 * X^T * Y\n",
        "W = np.linalg.inv(X_train_bias.T @ X_train_bias + lambda_identity) @ X_train_bias.T @ y_train_one_hot\n",
        "\n",
        "# Fazer previsões no conjunto de teste\n",
        "y_pred = X_test_bias @ W\n",
        "\n",
        "# Converter as probabilidades preditas para rótulos (classe com maior probabilidade)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Calcular a precisão\n",
        "accuracy = accuracy_score(y_test, y_pred_labels)\n",
        "print(f\"Acurácia: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Matriz de confusão\n",
        "#conf_matrix = confusion_matrix(y_test, y_pred_labels)\n",
        "#print(\"Matriz de confusão:\")\n",
        "#print(conf_matrix)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T02:24:27.535991Z",
          "iopub.execute_input": "2024-10-21T02:24:27.536565Z",
          "iopub.status.idle": "2024-10-21T02:24:39.599151Z",
          "shell.execute_reply.started": "2024-10-21T02:24:27.536511Z",
          "shell.execute_reply": "2024-10-21T02:24:39.595834Z"
        },
        "trusted": true,
        "id": "a33YuZepFZoY",
        "outputId": "89e0a62f-7f4d-421d-e58a-f55f71086b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Acurácia: 86.14%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A aplicação do PCA nos dados de entrada juntamente com a aplicação do modelo de classificação linear teve uma melhora na precisão em relação ao modelo sem PCA. Para isso, a retenção da variância que apresentou o melhor resultado foi de 94%."
      ],
      "metadata": {
        "id": "hcrDBcobFZoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| MODELOS | Acurácia |\n",
        "|------------|-------|\n",
        "| Classificador Linear | 86,03%    |\n",
        "| Madaline      | 86,58%    |\n",
        "| PCA + Classificador Linear | 86,14%    |\n",
        "| PCA + Madaline      | 88,48%    |"
      ],
      "metadata": {
        "id": "fjJnWBoVFZoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observando os modelos e comparado ao apresentado na questão, o modelo que apresentou resultado de valor superior foi o modelo PCA + Madaline, com uma Acurácia de 88,48%, enquanto o modelo de comparação tem um valor de 88%."
      ],
      "metadata": {
        "id": "SgGNJEkrFZoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP com neurônios ocultos definidos por PCA + Heuristica + random search"
      ],
      "metadata": {
        "id": "lLlFyY_OFZoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Carregar o dataset MNIST\n",
        "df_train = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')\n",
        "df_test = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_test.csv')\n",
        "\n",
        "# Separar rótulos e dados\n",
        "y_train = df_train.iloc[:, 0].values\n",
        "X_train = df_train.iloc[:, 1:].values #/ 255.0  # Normalizar\n",
        "y_test = df_test.iloc[:, 0].values\n",
        "X_test = df_test.iloc[:, 1:].values #/ 255.0\n",
        "\n",
        "# Codificação one-hot dos rótulos\n",
        "y_train_one_hot = to_categorical(y_train, 10)\n",
        "y_test_one_hot = to_categorical(y_test, 10)\n",
        "\n",
        "# Escalar os dados\n",
        "#scaler = StandardScaler()\n",
        "#X_train_scaled = scaler.fit_transform(X_train)\n",
        "#X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Definir função para criar MLP com número variável de neurônios ocultos\n",
        "def create_mlp(hidden_neurons, input_shape, output_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hidden_neurons, input_dim=input_shape, activation='sigmoid'))  # Camada oculta\n",
        "    model.add(Dense(output_shape, activation='softmax'))  # Camada de saída (10 classes)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T03:21:52.721100Z",
          "iopub.execute_input": "2024-10-21T03:21:52.722402Z",
          "iopub.status.idle": "2024-10-21T03:22:13.158917Z",
          "shell.execute_reply.started": "2024-10-21T03:21:52.722307Z",
          "shell.execute_reply": "2024-10-21T03:22:13.157647Z"
        },
        "trusted": true,
        "id": "imaKRKosFZoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A efeito de comparação com a questão, que não utiliza pre processamento,\n",
        "os modelos testados seguem o mesmo padrão.\n",
        "\n",
        "O modelo foi desenvolvido utilizando a biblioteca Tensorflow com uma camada oculta que possui a função de ativação sigmoidal, com o intuito de introduzir não linearidade ao modelo, e a camada de saída com a função de ativação\n",
        "softmax, ela é utilizada para modelos de classificação multiclasse para a transformação da saída em uma distribuição de probabilidade.\n",
        "\n",
        "Durante os testes com os modelos foram utilizados os otimizadores SGD e Adam, a utilização do otimizador Adam foi mantido pois o mesmo elevava a acurácia em 10% quando comparado ao SGD.\n"
      ],
      "metadata": {
        "id": "ubgRREU1F5_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste 1: Heurística básica - Regra do valor médio\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = 10  # Número de classes (10 dígitos)\n",
        "\n",
        "neurons_heuristic = input_dim + output_dim// 2\n",
        "model_heuristic = create_mlp(neurons_heuristic, input_dim, output_dim)\n",
        "history_heuristic = model_heuristic.fit(X_train, y_train_one_hot, epochs=10, batch_size=32, validation_data=(X_test, y_test_one_hot))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T03:23:54.984291Z",
          "iopub.execute_input": "2024-10-21T03:23:54.984760Z",
          "iopub.status.idle": "2024-10-21T03:26:07.834472Z",
          "shell.execute_reply.started": "2024-10-21T03:23:54.984719Z",
          "shell.execute_reply": "2024-10-21T03:26:07.833265Z"
        },
        "trusted": true,
        "id": "6KOA49FuFZoZ",
        "outputId": "86943296-1cc3-4fd9-fc3d-f08cb51da2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.8490 - loss: 0.0239 - val_accuracy: 0.9110 - val_loss: 0.0143\nEpoch 2/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9114 - loss: 0.0143 - val_accuracy: 0.9258 - val_loss: 0.0119\nEpoch 3/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9213 - loss: 0.0125 - val_accuracy: 0.9233 - val_loss: 0.0122\nEpoch 4/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9244 - loss: 0.0118 - val_accuracy: 0.9239 - val_loss: 0.0116\nEpoch 5/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9284 - loss: 0.0111 - val_accuracy: 0.9208 - val_loss: 0.0119\nEpoch 6/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9297 - loss: 0.0109 - val_accuracy: 0.9303 - val_loss: 0.0106\nEpoch 7/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9322 - loss: 0.0103 - val_accuracy: 0.9256 - val_loss: 0.0113\nEpoch 8/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9283 - loss: 0.0109 - val_accuracy: 0.9315 - val_loss: 0.0107\nEpoch 9/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9327 - loss: 0.0102 - val_accuracy: 0.9297 - val_loss: 0.0108\nEpoch 10/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9346 - loss: 0.0099 - val_accuracy: 0.9275 - val_loss: 0.0112\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste 1: Heurística básica - Regra da raiz quadrada\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = 10  # Número de classes (10 dígitos)\n",
        "\n",
        "neurons_heuristic = round(np.sqrt(input_dim + output_dim))\n",
        "model_heuristic = create_mlp(neurons_heuristic, input_dim, output_dim)\n",
        "history_heuristic = model_heuristic.fit(X_train, y_train_one_hot, epochs=10, batch_size=32, validation_data=(X_test, y_test_one_hot))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T03:26:23.598415Z",
          "iopub.execute_input": "2024-10-21T03:26:23.598838Z",
          "iopub.status.idle": "2024-10-21T03:27:01.625101Z",
          "shell.execute_reply.started": "2024-10-21T03:26:23.598799Z",
          "shell.execute_reply": "2024-10-21T03:27:01.623728Z"
        },
        "trusted": true,
        "id": "eipbbKBEFZoZ",
        "outputId": "e26de001-f1a1-42fe-fa4f-1d94b94f9f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.6364 - loss: 0.0526 - val_accuracy: 0.8443 - val_loss: 0.0252\nEpoch 2/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8549 - loss: 0.0230 - val_accuracy: 0.8723 - val_loss: 0.0208\nEpoch 3/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8667 - loss: 0.0206 - val_accuracy: 0.8772 - val_loss: 0.0193\nEpoch 4/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8698 - loss: 0.0198 - val_accuracy: 0.8780 - val_loss: 0.0186\nEpoch 5/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8751 - loss: 0.0189 - val_accuracy: 0.8799 - val_loss: 0.0183\nEpoch 6/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8711 - loss: 0.0192 - val_accuracy: 0.8837 - val_loss: 0.0174\nEpoch 7/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8794 - loss: 0.0181 - val_accuracy: 0.8855 - val_loss: 0.0174\nEpoch 8/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8809 - loss: 0.0180 - val_accuracy: 0.9000 - val_loss: 0.0151\nEpoch 9/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8876 - loss: 0.0168 - val_accuracy: 0.8897 - val_loss: 0.0166\nEpoch 10/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8771 - loss: 0.0182 - val_accuracy: 0.8920 - val_loss: 0.0165\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste 1: Heurística básica - Regra de Kolgomorov\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = 10  # Número de classes (10 dígitos)\n",
        "\n",
        "neurons_heuristic = 2 *input_dim + 1\n",
        "model_heuristic = create_mlp(neurons_heuristic, input_dim, output_dim)\n",
        "history_heuristic = model_heuristic.fit(X_train, y_train_one_hot, epochs=10, batch_size=32, validation_data=(X_test, y_test_one_hot))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T03:29:26.058827Z",
          "iopub.execute_input": "2024-10-21T03:29:26.059343Z",
          "iopub.status.idle": "2024-10-21T03:32:57.833177Z",
          "shell.execute_reply.started": "2024-10-21T03:29:26.059296Z",
          "shell.execute_reply": "2024-10-21T03:32:57.832008Z"
        },
        "trusted": true,
        "id": "Ikswqt5UFZoa",
        "outputId": "14b897a6-de09-43c4-eb3e-86964b10cf2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 11ms/step - accuracy: 0.8040 - loss: 0.0289 - val_accuracy: 0.9142 - val_loss: 0.0133\nEpoch 2/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9119 - loss: 0.0140 - val_accuracy: 0.9258 - val_loss: 0.0116\nEpoch 3/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9176 - loss: 0.0127 - val_accuracy: 0.9192 - val_loss: 0.0123\nEpoch 4/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9219 - loss: 0.0118 - val_accuracy: 0.9260 - val_loss: 0.0112\nEpoch 5/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9280 - loss: 0.0111 - val_accuracy: 0.9295 - val_loss: 0.0108\nEpoch 6/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9279 - loss: 0.0109 - val_accuracy: 0.9350 - val_loss: 0.0100\nEpoch 7/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - accuracy: 0.9289 - loss: 0.0108 - val_accuracy: 0.9330 - val_loss: 0.0103\nEpoch 8/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9362 - loss: 0.0099 - val_accuracy: 0.9377 - val_loss: 0.0096\nEpoch 9/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9345 - loss: 0.0100 - val_accuracy: 0.9270 - val_loss: 0.0111\nEpoch 10/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 11ms/step - accuracy: 0.9283 - loss: 0.0108 - val_accuracy: 0.9358 - val_loss: 0.0095\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste 2: Baseado em PCA para estimar o número de neurônios ocultos\n",
        "pca = PCA(n_components=0.95)  # Retenha 95% da variância\n",
        "pca.fit(X_train)\n",
        "n_pca_components = pca.n_components_  # Número de componentes principais\n",
        "\n",
        "model_pca = create_mlp(n_pca_components, input_dim, output_dim)\n",
        "history_pca = model_pca.fit(X_train, y_train_one_hot, epochs=10, batch_size=32, validation_data=(X_test, y_test_one_hot))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T03:33:56.066878Z",
          "iopub.execute_input": "2024-10-21T03:33:56.067328Z",
          "iopub.status.idle": "2024-10-21T03:35:02.808251Z",
          "shell.execute_reply.started": "2024-10-21T03:33:56.067288Z",
          "shell.execute_reply": "2024-10-21T03:35:02.806942Z"
        },
        "trusted": true,
        "id": "LUkJbp0lFZoa",
        "outputId": "b5345fcb-202d-49de-fba1-ff27f290d863"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7895 - loss: 0.0329 - val_accuracy: 0.8995 - val_loss: 0.0167\nEpoch 2/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8990 - loss: 0.0165 - val_accuracy: 0.9074 - val_loss: 0.0149\nEpoch 3/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8997 - loss: 0.0155 - val_accuracy: 0.9110 - val_loss: 0.0139\nEpoch 4/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9056 - loss: 0.0146 - val_accuracy: 0.9096 - val_loss: 0.0139\nEpoch 5/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9103 - loss: 0.0138 - val_accuracy: 0.9152 - val_loss: 0.0132\nEpoch 6/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9120 - loss: 0.0134 - val_accuracy: 0.9172 - val_loss: 0.0127\nEpoch 7/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9169 - loss: 0.0127 - val_accuracy: 0.9187 - val_loss: 0.0124\nEpoch 8/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9220 - loss: 0.0120 - val_accuracy: 0.9194 - val_loss: 0.0120\nEpoch 9/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9235 - loss: 0.0119 - val_accuracy: 0.9295 - val_loss: 0.0112\nEpoch 10/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9260 - loss: 0.0114 - val_accuracy: 0.9263 - val_loss: 0.0113\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste 3: Busca exaustiva - testar diferentes números de neurônios ocultos\n",
        "best_accuracy = 0\n",
        "best_neurons = 0\n",
        "for neurons in range(50, 401, 50):  # Testar de 50 a 400 neurônios ocultos\n",
        "    print(f\"Testando com {neurons} neurônios ocultos.\")\n",
        "    model = create_mlp(neurons, input_dim, output_dim)\n",
        "    history = model.fit(X_train, y_train_one_hot, epochs=10, batch_size=32, validation_data=(X_test, y_test_one_hot), verbose=0)\n",
        "    accuracy = history.history['val_accuracy'][-1]\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_neurons = neurons\n",
        "    print(f\"Validação final com {neurons} neurônios: Acurácia = {accuracy:.4f}\")\n",
        "\n",
        "print(f\"Melhor número de neurônios ocultos após busca exaustiva: {best_neurons} com acurácia de {best_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T03:35:53.241447Z",
          "iopub.execute_input": "2024-10-21T03:35:53.241947Z",
          "iopub.status.idle": "2024-10-21T03:44:29.711648Z",
          "shell.execute_reply.started": "2024-10-21T03:35:53.241898Z",
          "shell.execute_reply": "2024-10-21T03:44:29.709826Z"
        },
        "trusted": true,
        "id": "sZR7xsGnFZoa",
        "outputId": "23e7318c-3aa3-4c47-df70-c191c9ee33ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Testando com 50 neurônios ocultos.\nValidação final com 50 neurônios: Acurácia = 0.9158\nTestando com 100 neurônios ocultos.\nValidação final com 100 neurônios: Acurácia = 0.9125\nTestando com 150 neurônios ocultos.\nValidação final com 150 neurônios: Acurácia = 0.9252\nTestando com 200 neurônios ocultos.\nValidação final com 200 neurônios: Acurácia = 0.9250\nTestando com 250 neurônios ocultos.\nValidação final com 250 neurônios: Acurácia = 0.9175\nTestando com 300 neurônios ocultos.\nValidação final com 300 neurônios: Acurácia = 0.9243\nTestando com 350 neurônios ocultos.\nValidação final com 350 neurônios: Acurácia = 0.9347\nTestando com 400 neurônios ocultos.\nValidação final com 400 neurônios: Acurácia = 0.9277\nMelhor número de neurônios ocultos após busca exaustiva: 350 com acurácia de 0.9347\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O cálculo para a obtenção do valor correto de neurônios em uma camada oculta pode ser realizado de diversas formas, como observado nos testes acima. Pode-se considerar que esses cálculos servem como parâmetro inicial da quantidade de neurônios.\n",
        "\n",
        "Observando os testes realizados chegou ao resultado de melhor acurácia para a regra de Kolgomorov, esta calcula o numero de nuerônios na camada oculta como 2 vezes o número de entradas na rede mais 1, essa aplicação chegou a uma precisão de 93,58%.\n",
        "\n",
        "A tabela abaixo mostra os resultados para cada modelo testado com diferentes ténicas de obtenção da quantidade de neurônio."
      ],
      "metadata": {
        "id": "DX2nGPQQFZoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| MODELOS | Acurácia |\n",
        "|------------|-------|\n",
        "| Regra do valor médio | 92,75%    |\n",
        "| Regra da raiz quadrada      | 89,20%    |\n",
        "| Kolgomorov | 93,58%    |\n",
        "| PCA       | 92,63%    |\n",
        "| Busca Exaustiva (250 neurônios)       | 93,47%    |"
      ],
      "metadata": {
        "id": "UtjkuBSdFZoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apesar disso, o resultado dessas técnicas ainda ficaram abaixo do valor da precisão do modelo utilizado como comparação na questão, que possui o valor de 95,3% nos dados de teste."
      ],
      "metadata": {
        "id": "XWqIoMQ_FZoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA+MLP com q neurônios ocultos\n"
      ],
      "metadata": {
        "id": "bWZYN4e-FZoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Carregar o dataset MNIST\n",
        "df_train = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')\n",
        "df_test = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_test.csv')\n",
        "\n",
        "# Separar rótulos e dados\n",
        "y_train = df_train.iloc[:, 0].values\n",
        "X_train = df_train.iloc[:, 1:].values  # Dados de entrada\n",
        "y_test = df_test.iloc[:, 0].values\n",
        "X_test = df_test.iloc[:, 1:].values  # Dados de entrada\n",
        "\n",
        "# Codificação one-hot dos rótulos\n",
        "y_train_one_hot = to_categorical(y_train, 10)\n",
        "y_test_one_hot = to_categorical(y_test, 10)\n",
        "\n",
        "# Normalizar os dados\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Aplicar PCA para retenção de 95% da variância\n",
        "pca = PCA(0.95)  # Manter 95% da variância\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Verificar a quantidade de componentes principais retidas\n",
        "print(f\"Número de componentes principais retidas: {X_train_pca.shape[1]}\")\n",
        "\n",
        "# Definir função para criar MLP com número variável de neurônios ocultos\n",
        "def create_mlp(hidden_neurons, input_shape, output_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hidden_neurons, input_dim=input_shape, activation='sigmoid'))  # Camada oculta\n",
        "    model.add(Dense(output_shape, activation='softmax'))  # Camada de saída (10 classes)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Teste 1: Heurística básica - Regra de Kolmogorov\n",
        "input_dim = X_train_pca.shape[1]  # Atualizar a dimensão de entrada para as componentes principais\n",
        "output_dim = 10  # Número de classes (10 dígitos)\n",
        "\n",
        "neurons_heuristic = 2 * input_dim + 1\n",
        "model_heuristic = create_mlp(neurons_heuristic, input_dim, output_dim)\n",
        "history_heuristic = model_heuristic.fit(X_train_pca, y_train_one_hot, epochs=10, batch_size=32, validation_data=(X_test_pca, y_test_one_hot))\n",
        "\n",
        "# Avaliar o modelo\n",
        "loss, accuracy = model_heuristic.evaluate(X_test_pca, y_test_one_hot)\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T04:06:31.547748Z",
          "iopub.execute_input": "2024-10-21T04:06:31.548909Z",
          "iopub.status.idle": "2024-10-21T04:08:07.820020Z",
          "shell.execute_reply.started": "2024-10-21T04:06:31.548844Z",
          "shell.execute_reply": "2024-10-21T04:08:07.818463Z"
        },
        "trusted": true,
        "id": "WPB7D4_hFZoa",
        "outputId": "cbcfe629-49ce-47c6-959b-e3529908e92b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Número de componentes principais retidas: 331\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8557 - loss: 0.0216 - val_accuracy: 0.9305 - val_loss: 0.0106\nEpoch 2/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9377 - loss: 0.0096 - val_accuracy: 0.9401 - val_loss: 0.0090\nEpoch 3/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9537 - loss: 0.0072 - val_accuracy: 0.9503 - val_loss: 0.0076\nEpoch 4/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9643 - loss: 0.0056 - val_accuracy: 0.9558 - val_loss: 0.0066\nEpoch 5/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9729 - loss: 0.0045 - val_accuracy: 0.9608 - val_loss: 0.0060\nEpoch 6/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9788 - loss: 0.0034 - val_accuracy: 0.9630 - val_loss: 0.0058\nEpoch 7/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9833 - loss: 0.0027 - val_accuracy: 0.9667 - val_loss: 0.0051\nEpoch 8/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9866 - loss: 0.0023 - val_accuracy: 0.9683 - val_loss: 0.0051\nEpoch 9/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9888 - loss: 0.0019 - val_accuracy: 0.9679 - val_loss: 0.0050\nEpoch 10/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9896 - loss: 0.0018 - val_accuracy: 0.9687 - val_loss: 0.0049\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9641 - loss: 0.0055\nLoss: 0.004872738383710384, Accuracy: 0.9686999917030334\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ao observar a aplicação do modelo de PCA + MLP com o número de neurônios ocultos definidos através da regra de Kolgomorov e a aplicação em 10 epocas, nota-se a obtenção de uma precisão de  aproximadamente 96,86%, resultado superior ao modelo utilizado como comparação da questão 1, no valor de 95,3%."
      ],
      "metadata": {
        "id": "d-dFHPJhFZob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder + Adaline c/ Função Sigmoidal"
      ],
      "metadata": {
        "id": "k5fa348ZFZob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Carregar os dataframes\n",
        "df_train = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')\n",
        "df_test = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_test.csv')\n",
        "\n",
        "# Separar os rótulos e dados de treino\n",
        "y_train = df_train.iloc[:, 0].values  # A primeira coluna contém os rótulos\n",
        "X_train = df_train.iloc[:, 1:].values / 255.0  # Normalização\n",
        "X_test = df_test.iloc[:, 1:].values / 255.0  # Normalização\n",
        "\n",
        "# Separar os rótulos e dados de teste\n",
        "y_test = df_test.iloc[:, 0].values  # A primeira coluna contém os rótulos\n",
        "\n",
        "# Codificação one-hot para os rótulos\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_one_hot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_one_hot = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Definir o autoencoder\n",
        "def create_autoencoder(input_shape, encoding_dim):\n",
        "    input_layer = Input(shape=(input_shape,))\n",
        "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "    decoded = Dense(input_shape, activation='sigmoid')(encoded)\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "    encoder = Model(input_layer, encoded)  # Modelo encoder\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return autoencoder, encoder\n",
        "\n",
        "# Definir dimensão de codificação\n",
        "encoding_dim = 256  # Dimensão do espaço latente\n",
        "\n",
        "# Criar e treinar o autoencoder e o encoder\n",
        "autoencoder, encoder_model = create_autoencoder(X_train.shape[1], encoding_dim)\n",
        "autoencoder.fit(X_train, X_train, epochs=50, batch_size=256, validation_data=(X_test, X_test))\n",
        "\n",
        "# Obter as representações codificadas\n",
        "X_train_encoded = encoder_model.predict(X_train)\n",
        "X_test_encoded = encoder_model.predict(X_test)\n",
        "\n",
        "# Confirmar as formas das saídas codificadas\n",
        "print(f'X_train_encoded shape: {X_train_encoded.shape}')  # Deve ser (num_samples, 64)\n",
        "print(f'X_test_encoded shape: {X_test_encoded.shape}')    # Deve ser (num_samples, 64)\n",
        "\n",
        "# Função sigmoide\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Função para treinar o modelo MADALINE com função de ativação sigmoide\n",
        "def train_madaline_sigmoid(X, y, epochs=1000, lr=0.01):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_classes = y.shape[1]\n",
        "\n",
        "    # Inicializar os pesos\n",
        "    weights = np.random.rand(n_classes, n_features)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        z_input = np.dot(X, weights.T)  # Saídas lineares\n",
        "        z = sigmoid(z_input)  # Função sigmoide\n",
        "\n",
        "        # Calcular o erro\n",
        "        error = y - z\n",
        "\n",
        "        # Atualizar os pesos\n",
        "        for j in range(n_classes):\n",
        "            mask = (y[:, j] > 0) & (z[:, j] <= 0.5) | (y[:, j] <= 0) & (z[:, j] > 0.5)\n",
        "            weights[j] += lr * np.dot(mask * error[:, j], X)  # Atualizar pesos\n",
        "\n",
        "        # Imprimir erro por época\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}: Error = {np.sum(np.square(error)):.4f}\")\n",
        "\n",
        "    return weights\n",
        "\n",
        "# Treinar o modelo MADALINE com dados codificados\n",
        "weights = train_madaline_sigmoid(X_train_encoded, y_train_one_hot, epochs=1000, lr=0.1)\n",
        "\n",
        "# Fazer previsões no conjunto de teste\n",
        "y_pred_prob = sigmoid(X_test_encoded @ weights.T)\n",
        "y_pred_labels = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "# Calcular a acurácia\n",
        "accuracy = accuracy_score(y_test, y_pred_labels)\n",
        "print(f\"Acurácia: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T05:50:06.709049Z",
          "iopub.execute_input": "2024-10-21T05:50:06.709662Z",
          "iopub.status.idle": "2024-10-21T06:06:22.831045Z",
          "shell.execute_reply.started": "2024-10-21T05:50:06.709602Z",
          "shell.execute_reply": "2024-10-21T06:06:22.828779Z"
        },
        "trusted": true,
        "id": "HkT-d6nOFZob",
        "outputId": "1d63e7ad-0d95-4b70-b271-8eb34e1d564c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.2805 - val_loss: 0.1106\nEpoch 2/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.1032 - val_loss: 0.0857\nEpoch 3/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0837 - val_loss: 0.0766\nEpoch 4/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0761 - val_loss: 0.0723\nEpoch 5/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0722 - val_loss: 0.0701\nEpoch 6/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0702 - val_loss: 0.0687\nEpoch 7/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0689 - val_loss: 0.0678\nEpoch 8/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0681 - val_loss: 0.0671\nEpoch 9/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0674 - val_loss: 0.0666\nEpoch 10/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0667 - val_loss: 0.0663\nEpoch 11/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0665 - val_loss: 0.0659\nEpoch 12/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0662 - val_loss: 0.0657\nEpoch 13/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0657 - val_loss: 0.0655\nEpoch 14/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0657 - val_loss: 0.0653\nEpoch 15/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0654 - val_loss: 0.0651\nEpoch 16/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0653 - val_loss: 0.0649\nEpoch 17/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0652 - val_loss: 0.0649\nEpoch 18/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0650 - val_loss: 0.0648\nEpoch 19/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0648 - val_loss: 0.0646\nEpoch 20/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0647 - val_loss: 0.0645\nEpoch 21/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0647 - val_loss: 0.0645\nEpoch 22/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0647 - val_loss: 0.0644\nEpoch 23/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0645 - val_loss: 0.0644\nEpoch 24/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0644 - val_loss: 0.0643\nEpoch 25/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0644 - val_loss: 0.0643\nEpoch 26/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0643 - val_loss: 0.0642\nEpoch 27/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0643 - val_loss: 0.0642\nEpoch 28/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0642 - val_loss: 0.0642\nEpoch 29/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0642 - val_loss: 0.0641\nEpoch 30/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0641 - val_loss: 0.0641\nEpoch 31/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0642 - val_loss: 0.0641\nEpoch 32/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0641 - val_loss: 0.0640\nEpoch 33/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0641 - val_loss: 0.0640\nEpoch 34/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0640 - val_loss: 0.0639\nEpoch 35/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0640 - val_loss: 0.0639\nEpoch 36/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0640 - val_loss: 0.0639\nEpoch 37/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0640 - val_loss: 0.0639\nEpoch 38/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0639 - val_loss: 0.0639\nEpoch 39/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0638 - val_loss: 0.0638\nEpoch 40/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0639 - val_loss: 0.0638\nEpoch 41/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0639 - val_loss: 0.0638\nEpoch 42/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0638 - val_loss: 0.0638\nEpoch 43/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0638 - val_loss: 0.0638\nEpoch 44/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0638 - val_loss: 0.0638\nEpoch 45/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0638 - val_loss: 0.0638\nEpoch 46/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0639 - val_loss: 0.0637\nEpoch 47/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0637 - val_loss: 0.0638\nEpoch 48/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0638 - val_loss: 0.0638\nEpoch 49/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0638 - val_loss: 0.0638\nEpoch 50/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0638 - val_loss: 0.0637\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\nX_train_encoded shape: (60000, 256)\nX_test_encoded shape: (10000, 256)\nEpoch 0: Error = 540000.0000\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_37/917502392.py:52: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-x))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 100: Error = 36102.9937\nEpoch 200: Error = 32336.8165\nEpoch 300: Error = 42714.9516\nEpoch 400: Error = 16907.9999\nEpoch 500: Error = 19831.2083\nEpoch 600: Error = 21768.0000\nEpoch 700: Error = 23233.9889\nEpoch 800: Error = 20082.0058\nEpoch 900: Error = 16976.0000\nAcurácia: 86.76%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Foram realizadas modificações na dimensão do espaço latente do autoencoder, utilizando 64, 128 e 256, porém nenhum dos casos surtiu efeito positivo na precisão do modelo.\n",
        "\n",
        "Nos casos com valor 64 e 128 ocorreram uma queda na precisão, que no modelo multi Adaline tem uma precisão de 86%, no modelo PCA + multi Adaline com uma precisão de 88%, e agora tiveram resultados de aproximadamente 79% e 82%, respectivamente.\n",
        "\n",
        "O modelo utilizando o valor latente apresentou um resultado de Acurácia maior, porém próximo ao modelo MADALINE sem qualquer uso de pré processamento."
      ],
      "metadata": {
        "id": "zS3xMv8vFZob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder + MLP"
      ],
      "metadata": {
        "id": "ODlHbB_vFZob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Carregar o dataset MNIST a partir de arquivos CSV\n",
        "df_train = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')\n",
        "df_test = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_test.csv')\n",
        "\n",
        "# Separar rótulos e dados\n",
        "y_train = df_train.iloc[:, 0].values  # A primeira coluna contém os rótulos (dígitos)\n",
        "X_train = df_train.iloc[:, 1:].values  # As colunas subsequentes contêm os dados de entrada (imagens achatadas)\n",
        "y_test = df_test.iloc[:, 0].values  # A primeira coluna contém os rótulos (dígitos)\n",
        "X_test = df_test.iloc[:, 1:].values  # As colunas subsequentes contêm os dados de entrada (imagens achatadas)\n",
        "\n",
        "# Normalizar os dados para o intervalo [0, 1]\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Codificação one-hot dos rótulos (necessária para classificação)\n",
        "y_train_one_hot = to_categorical(y_train, 10)\n",
        "y_test_one_hot = to_categorical(y_test, 10)\n",
        "\n",
        "# Definir o autoencoder\n",
        "def create_autoencoder(input_shape, encoding_dim):\n",
        "    input_layer = Input(shape=(input_shape,))\n",
        "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "    decoded = Dense(input_shape, activation='sigmoid')(encoded)\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "    encoder = Model(input_layer, encoded)  # Aqui definimos o modelo encoder\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return autoencoder, encoder\n",
        "\n",
        "# Definir dimensão de codificação\n",
        "encoding_dim = 64  # Dimensão do espaço latente\n",
        "\n",
        "# Criar e treinar o autoencoder e o encoder\n",
        "autoencoder, encoder = create_autoencoder(X_train.shape[1], encoding_dim)\n",
        "autoencoder.fit(X_train, X_train, epochs=50, batch_size=256, validation_data=(X_test, X_test))\n",
        "\n",
        "# Obter as representações codificadas\n",
        "X_train_encoded = encoder.predict(X_train)\n",
        "X_test_encoded = encoder.predict(X_test)\n",
        "\n",
        "# Confirmar as formas das saídas codificadas\n",
        "print(f'X_train_encoded shape: {X_train_encoded.shape}')  # Deve ser (num_samples, 64)\n",
        "print(f'X_test_encoded shape: {X_test_encoded.shape}')    # Deve ser (num_samples, 64)\n",
        "\n",
        "# Definir função para criar MLP\n",
        "def create_mlp(hidden_neurons, input_shape, output_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hidden_neurons, input_dim=input_shape, activation='relu'))  # Camada oculta\n",
        "    model.add(Dense(output_shape, activation='softmax'))  # Camada de saída (10 classes)\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Teste 1: Heurística básica - Regra de Kolmogorov\n",
        "input_dim = encoding_dim  # Atualizar a dimensão de entrada para o espaço latente do autoencoder\n",
        "output_dim = 10  # Número de classes (10 dígitos)\n",
        "neurons_heuristic = 2 * input_dim + 1\n",
        "model_heuristic = create_mlp(neurons_heuristic, input_dim, output_dim)\n",
        "\n",
        "# Treinar o modelo MLP com dados codificados\n",
        "history_heuristic = model_heuristic.fit(X_train_encoded, y_train_one_hot, epochs=10, batch_size=32, validation_data=(X_test_encoded, y_test_one_hot))\n",
        "\n",
        "# Avaliar o modelo\n",
        "loss, accuracy = model_heuristic.evaluate(X_test_encoded, y_test_one_hot)\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-21T05:16:34.707691Z",
          "iopub.execute_input": "2024-10-21T05:16:34.708200Z",
          "iopub.status.idle": "2024-10-21T05:19:36.006400Z",
          "shell.execute_reply.started": "2024-10-21T05:16:34.708152Z",
          "shell.execute_reply": "2024-10-21T05:19:36.005071Z"
        },
        "trusted": true,
        "id": "ce0PzXCSFZob",
        "outputId": "2b9bf8aa-b6ff-413b-8ed4-4dadda698774"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.3481 - val_loss: 0.1627\nEpoch 2/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.1529 - val_loss: 0.1266\nEpoch 3/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.1229 - val_loss: 0.1088\nEpoch 4/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.1069 - val_loss: 0.0980\nEpoch 5/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0968 - val_loss: 0.0904\nEpoch 6/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0901 - val_loss: 0.0853\nEpoch 7/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0852 - val_loss: 0.0819\nEpoch 8/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0820 - val_loss: 0.0795\nEpoch 9/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0799 - val_loss: 0.0778\nEpoch 10/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0781 - val_loss: 0.0766\nEpoch 11/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0772 - val_loss: 0.0758\nEpoch 12/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0761 - val_loss: 0.0751\nEpoch 13/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0758 - val_loss: 0.0747\nEpoch 14/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0753 - val_loss: 0.0744\nEpoch 15/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0751 - val_loss: 0.0741\nEpoch 16/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0746 - val_loss: 0.0739\nEpoch 17/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0745 - val_loss: 0.0737\nEpoch 18/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0743 - val_loss: 0.0736\nEpoch 19/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0742 - val_loss: 0.0734\nEpoch 20/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0740 - val_loss: 0.0733\nEpoch 21/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0740 - val_loss: 0.0733\nEpoch 22/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0738 - val_loss: 0.0732\nEpoch 23/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0737 - val_loss: 0.0731\nEpoch 24/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0737 - val_loss: 0.0731\nEpoch 25/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0737 - val_loss: 0.0730\nEpoch 26/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0736 - val_loss: 0.0730\nEpoch 27/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0734 - val_loss: 0.0729\nEpoch 28/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0734 - val_loss: 0.0729\nEpoch 29/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0734 - val_loss: 0.0728\nEpoch 30/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0734 - val_loss: 0.0728\nEpoch 31/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0734 - val_loss: 0.0728\nEpoch 32/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0735 - val_loss: 0.0727\nEpoch 33/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0733 - val_loss: 0.0728\nEpoch 34/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0734 - val_loss: 0.0727\nEpoch 35/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0731 - val_loss: 0.0727\nEpoch 36/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0732 - val_loss: 0.0727\nEpoch 37/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0733 - val_loss: 0.0727\nEpoch 38/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0731 - val_loss: 0.0727\nEpoch 39/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0731 - val_loss: 0.0727\nEpoch 40/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0730 - val_loss: 0.0726\nEpoch 41/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0730 - val_loss: 0.0726\nEpoch 42/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0731 - val_loss: 0.0726\nEpoch 43/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0731 - val_loss: 0.0726\nEpoch 44/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0731 - val_loss: 0.0726\nEpoch 45/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0730 - val_loss: 0.0726\nEpoch 46/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0730 - val_loss: 0.0726\nEpoch 47/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0731 - val_loss: 0.0726\nEpoch 48/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0730 - val_loss: 0.0725\nEpoch 49/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0730 - val_loss: 0.0726\nEpoch 50/50\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0731 - val_loss: 0.0725\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\nX_train_encoded shape: (60000, 64)\nX_test_encoded shape: (10000, 64)\nEpoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.6973 - loss: 0.0426 - val_accuracy: 0.9197 - val_loss: 0.0122\nEpoch 2/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9139 - loss: 0.0131 - val_accuracy: 0.9250 - val_loss: 0.0115\nEpoch 3/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9302 - loss: 0.0107 - val_accuracy: 0.9336 - val_loss: 0.0101\nEpoch 4/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9425 - loss: 0.0088 - val_accuracy: 0.9426 - val_loss: 0.0089\nEpoch 5/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9501 - loss: 0.0077 - val_accuracy: 0.9387 - val_loss: 0.0093\nEpoch 6/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9557 - loss: 0.0068 - val_accuracy: 0.9581 - val_loss: 0.0067\nEpoch 7/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.0065 - val_accuracy: 0.9602 - val_loss: 0.0062\nEpoch 8/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9620 - loss: 0.0060 - val_accuracy: 0.9609 - val_loss: 0.0060\nEpoch 9/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9625 - loss: 0.0058 - val_accuracy: 0.9565 - val_loss: 0.0067\nEpoch 10/10\n\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9661 - loss: 0.0054 - val_accuracy: 0.9643 - val_loss: 0.0057\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9562 - loss: 0.0069\nLoss: 0.005704780109226704, Accuracy: 0.9642999768257141\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O algoritmo utilizando autoencoder para redução de dimensionalidade + o MLP desenvolvido com os modelos apresentados anteriormente obteve o resultado de precisão de 96,43% para os dados de teste, valor bem próximo ao encontrado utilizando PCA + MLP e melhor do que o apresentado no modelo de comparação da questão 1."
      ],
      "metadata": {
        "id": "Mdm3pobMFZob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|Modelos|Acurácia|\n",
        "|-------|--------|\n",
        "|linear classifier (1-layer NN) - Yann Lecun\t|88%|\n",
        "|Classificador Linear MMQ|86,03%|\n",
        "|Adaline com Função de Ativação Sigmoidal|86,58%|\n",
        "|PCA + Classificador linear com Minimos Quadrados|86,14%|\n",
        "|PCA com modelo Adaline e função de ativação Sigmoidal|88,48%|\n",
        "|Autoencoder + Adaline c/ Função Sigmoidal|86,76%|\n",
        "|2-layer NN, 300 hidden units, mean square error - Yann Lecun|95,3%|\n",
        "|MLP com neuronios ocultos definidos por PCA ou Heuristica ou random search|----|\n",
        "| Regra do valor médio(Heurística)| 92,75%|\n",
        "| Regra da raiz quadrada(Heurística)| 89,20%|\n",
        "| Kolgomorov(Heurística)| 93,58%|\n",
        "| PCA       | 92,63%|\n",
        "| Busca Exaustiva (250 neurônios)| 93,47%|\n",
        "|PCA + MLP + regra de Kolgomorov |96,86%|\n",
        "|Autoencoder + MLP|96,42%|\n"
      ],
      "metadata": {
        "id": "Ui_LL7xIGpcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observando todos os modelos desenvolvidos e comparados com os modelos que são base para comparação, em relação ao classificador linear, apenas o modelo PCA + Adaline conseguiu valor superior, os outros modelos tiveram valores próximos mas nao abaixo do modelo de Yann Lecun.\n",
        "\n",
        "Em relação aos modelos utilizando redes neurais, os modelos PCA + MLP utilizando a quantidade de neurônios da camada oculta usando Kolgomorov e o modelo usando Autoencoder e a regra de Kolgomorov apresentaram resultados superiores ao modelo de redes neurais utilizado como base de comparação.\n"
      ],
      "metadata": {
        "id": "w6YBgNlmKj1i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BVvrODalFZoc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}